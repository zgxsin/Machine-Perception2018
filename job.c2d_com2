2018-06-01 13:15:53.603314: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-06-01 13:15:54.122601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:04:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2018-06-01 13:15:54.122645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-06-01 13:15:54.465416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-01 13:15:54.465462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-06-01 13:15:54.465470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-06-01 13:15:54.466024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10409 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
2018-06-01 13:16:13.594775: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.55GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 13:16:21.118548: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.92GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 13:16:22.708470: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.38GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 13:16:28.728802: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.83GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 13:16:28.728906: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.32GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 13:16:31.136404: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.80GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 13:16:31.373240: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.88GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 13:16:31.467490: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.34GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 13:16:44.499124: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.07GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 13:16:46.026004: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.42GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
/cluster/apps/python/3.6.4/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Writing to /cluster/home/guzhou/MP2018_2D_compare/runs/lstm1_512_cnn5_drop3_5e4_avg_logit_1527851750


# of parameters: 3195268
[Train/50] Accuracy: 0.045, Loss: 3.063, time/step = 0.087
[Train/100] Accuracy: 0.051, Loss: 3.033, time/step = 0.050
[Train/150] Accuracy: 0.055, Loss: 3.023, time/step = 0.070
[Train/200] Accuracy: 0.061, Loss: 2.999, time/step = 0.082
[Train/250] Accuracy: 0.105, Loss: 2.885, time/step = 0.081
[Train/300] Accuracy: 0.153, Loss: 2.621, time/step = 0.069
[Train/350] Accuracy: 0.160, Loss: 2.555, time/step = 0.063
[Valid/380] Accuracy: 0.271, Loss: 2.275, time/step = 3.767
[Train/400] Accuracy: 0.208, Loss: 2.447, time/step = 0.129
[Train/450] Accuracy: 0.237, Loss: 2.335, time/step = 0.081
[Train/500] Accuracy: 0.280, Loss: 2.259, time/step = 0.095
[Train/550] Accuracy: 0.307, Loss: 2.165, time/step = 0.073
[Train/600] Accuracy: 0.330, Loss: 2.164, time/step = 0.055
[Train/650] Accuracy: 0.329, Loss: 2.184, time/step = 0.106
[Train/700] Accuracy: 0.357, Loss: 1.980, time/step = 0.077
[Train/750] Accuracy: 0.353, Loss: 2.055, time/step = 0.105
Traceback (most recent call last):
  File "training.py", line 303, in <module>
    main(config)
  File "training.py", line 260, in main
    validModel.loss])
  File "/cluster/apps/python/3.6.4/lib64/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/cluster/apps/python/3.6.4/lib64/python3.6/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/cluster/apps/python/3.6.4/lib64/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run
    run_metadata)
  File "/cluster/apps/python/3.6.4/lib64/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call
    return fn(*args)
  File "/cluster/apps/python/3.6.4/lib64/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/cluster/apps/python/3.6.4/lib64/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_tf_sessionrun
    status, run_metadata)
KeyboardInterrupt
terminate called without an active exception
2018-06-01 14:15:27.020772: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-06-01 14:15:27.533442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:04:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2018-06-01 14:15:27.533478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-06-01 14:15:27.842030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-01 14:15:27.842074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-06-01 14:15:27.842081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-06-01 14:15:27.842716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10409 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
2018-06-01 14:15:47.593468: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.71GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 14:15:47.672091: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.02GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 14:15:48.387502: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 14:15:48.698229: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.70GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 14:15:55.710059: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.42GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 14:16:04.846621: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.55GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 14:16:05.618100: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.92GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 14:16:15.376278: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.49GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 14:16:34.146878: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.67GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 14:16:34.221708: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.99GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-01 21:05:35.478341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-06-01 21:05:35.478635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-01 21:05:35.478647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-06-01 21:05:35.478653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-06-01 21:05:35.479148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10409 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
/cluster/apps/python/3.6.4/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Writing to /cluster/home/guzhou/MP2018_2D_compare/runs/lstm1_512_cnn5_drop3_5e4_avg_logit_1527855324


# of parameters: 3195268
[Train/50] Accuracy: 0.051, Loss: 3.067, time/step = 0.093
[Train/100] Accuracy: 0.049, Loss: 3.038, time/step = 0.093
[Train/150] Accuracy: 0.077, Loss: 2.983, time/step = 0.066
[Train/200] Accuracy: 0.101, Loss: 2.881, time/step = 0.085
[Train/250] Accuracy: 0.142, Loss: 2.660, time/step = 0.096
[Train/300] Accuracy: 0.200, Loss: 2.466, time/step = 0.093
[Train/350] Accuracy: 0.239, Loss: 2.385, time/step = 0.092
[Valid/380] Accuracy: 0.332, Loss: 2.106, time/step = 4.237
[Train/400] Accuracy: 0.256, Loss: 2.319, time/step = 0.023
[Train/450] Accuracy: 0.279, Loss: 2.206, time/step = 0.096
[Train/500] Accuracy: 0.324, Loss: 2.138, time/step = 0.093
[Train/550] Accuracy: 0.331, Loss: 2.204, time/step = 0.089
[Train/600] Accuracy: 0.357, Loss: 2.031, time/step = 0.095
[Train/650] Accuracy: 0.383, Loss: 1.900, time/step = 0.080
[Train/700] Accuracy: 0.465, Loss: 1.728, time/step = 0.093
[Train/750] Accuracy: 0.463, Loss: 1.766, time/step = 0.098
[Valid/760] Accuracy: 0.493, Loss: 1.680, time/step = 4.099
[Train/800] Accuracy: 0.486, Loss: 1.662, time/step = 0.090
[Train/850] Accuracy: 0.485, Loss: 1.674, time/step = 0.099
[Train/900] Accuracy: 0.515, Loss: 1.556, time/step = 0.088
Model saved in file: /cluster/home/guzhou/MP2018_2D_compare/runs/lstm1_512_cnn5_drop3_5e4_avg_logit_1527855324/model-950
[Train/950] Accuracy: 0.564, Loss: 1.436, time/step = 0.069
[Train/1000] Accuracy: 0.567, Loss: 1.454, time/step = 0.095
[Train/1050] Accuracy: 0.576, Loss: 1.390, time/step = 0.089
[Train/1100] Accuracy: 0.621, Loss: 1.238, time/step = 0.084
[Valid/1140] Accuracy: 0.576, Loss: 1.446, time/step = 4.055
[Train/1150] Accuracy: 0.633, Loss: 1.236, time/step = 0.021
[Train/1200] Accuracy: 0.619, Loss: 1.251, time/step = 0.092
[Train/1250] Accuracy: 0.601, Loss: 1.268, time/step = 0.087
[Train/1300] Accuracy: 0.687, Loss: 1.068, time/step = 0.088
[Train/1350] Accuracy: 0.655, Loss: 1.101, time/step = 0.086
[Train/1400] Accuracy: 0.699, Loss: 1.011, time/step = 0.084
[Train/1450] Accuracy: 0.706, Loss: 1.013, time/step = 0.090
[Train/1500] Accuracy: 0.709, Loss: 1.024, time/step = 0.094
[Valid/1520] Accuracy: 0.633, Loss: 1.318, time/step = 4.057
[Train/1550] Accuracy: 0.699, Loss: 1.035, time/step = 0.185
[Train/1600] Accuracy: 0.749, Loss: 0.871, time/step = 0.088
[Train/1650] Accuracy: 0.734, Loss: 0.920, time/step = 0.086
[Train/1700] Accuracy: 0.737, Loss: 0.907, time/step = 0.102
[Train/1750] Accuracy: 0.736, Loss: 0.915, time/step = 0.093
[Train/1800] Accuracy: 0.777, Loss: 0.836, time/step = 0.090
[Train/1850] Accuracy: 0.771, Loss: 0.771, time/step = 0.088
Model saved in file: /cluster/home/guzhou/MP2018_2D_compare/runs/lstm1_512_cnn5_drop3_5e4_avg_logit_1527855324/model-1900
[Train/1900] Accuracy: 0.777, Loss: 0.770, time/step = 0.067
[Valid/1900] Accuracy: 0.671, Loss: 1.244, time/step = 4.067
[Train/1950] Accuracy: 0.836, Loss: 0.604, time/step = 0.092
[Train/2000] Accuracy: 0.767, Loss: 0.800, time/step = 0.093
[Train/2050] Accuracy: 0.778, Loss: 0.815, time/step = 0.087
[Train/2100] Accuracy: 0.819, Loss: 0.672, time/step = 0.094
[Train/2150] Accuracy: 0.815, Loss: 0.659, time/step = 0.099
[Train/2200] Accuracy: 0.851, Loss: 0.619, time/step = 0.086
[Train/2250] Accuracy: 0.813, Loss: 0.724, time/step = 0.090
[Valid/2280] Accuracy: 0.687, Loss: 1.283, time/step = 4.135
[Train/2300] Accuracy: 0.856, Loss: 0.559, time/step = 0.020
[Train/2350] Accuracy: 0.874, Loss: 0.498, time/step = 0.084
[Train/2400] Accuracy: 0.848, Loss: 0.577, time/step = 0.087
[Train/2450] Accuracy: 0.833, Loss: 0.673, time/step = 0.091
[Train/2500] Accuracy: 0.865, Loss: 0.543, time/step = 0.083
[Train/2550] Accuracy: 0.886, Loss: 0.472, time/step = 0.090
[Train/2600] Accuracy: 0.848, Loss: 0.564, time/step = 0.091
[Train/2650] Accuracy: 0.865, Loss: 0.518, time/step = 0.089
[Valid/2660] Accuracy: 0.701, Loss: 1.280, time/step = 4.042
[Train/2700] Accuracy: 0.903, Loss: 0.438, time/step = 0.092
[Train/2750] Accuracy: 0.891, Loss: 0.455, time/step = 0.093
[Train/2800] Accuracy: 0.877, Loss: 0.495, time/step = 0.095
Model saved in file: /cluster/home/guzhou/MP2018_2D_compare/runs/lstm1_512_cnn5_drop3_5e4_avg_logit_1527855324/model-2850
[Train/2850] Accuracy: 0.881, Loss: 0.485, time/step = 0.071
[Train/2900] Accuracy: 0.914, Loss: 0.412, time/step = 0.096
[Train/2950] Accuracy: 0.900, Loss: 0.429, time/step = 0.094
[Train/3000] Accuracy: 0.890, Loss: 0.460, time/step = 0.100
[Valid/3040] Accuracy: 0.694, Loss: 1.332, time/step = 3.992
[Train/3050] Accuracy: 0.908, Loss: 0.424, time/step = 0.021
[Train/3100] Accuracy: 0.927, Loss: 0.375, time/step = 0.097
[Train/3150] Accuracy: 0.903, Loss: 0.425, time/step = 0.093
[Train/3200] Accuracy: 0.955, Loss: 0.284, time/step = 0.104
[Train/3250] Accuracy: 0.912, Loss: 0.379, time/step = 0.093
[Train/3300] Accuracy: 0.951, Loss: 0.302, time/step = 0.086
[Train/3350] Accuracy: 0.951, Loss: 0.300, time/step = 0.093
[Train/3400] Accuracy: 0.907, Loss: 0.399, time/step = 0.094
[Valid/3420] Accuracy: 0.708, Loss: 1.359, time/step = 4.081
[Train/3450] Accuracy: 0.911, Loss: 0.384, time/step = 0.175
[Train/3500] Accuracy: 0.921, Loss: 0.384, time/step = 0.089
[Train/3550] Accuracy: 0.938, Loss: 0.336, time/step = 0.088
[Train/3600] Accuracy: 0.914, Loss: 0.377, time/step = 0.086
[Train/3650] Accuracy: 0.949, Loss: 0.313, time/step = 0.083
[Train/3700] Accuracy: 0.949, Loss: 0.294, time/step = 0.085
[Train/3750] Accuracy: 0.943, Loss: 0.328, time/step = 0.091
Model saved in file: /cluster/home/guzhou/MP2018_2D_compare/runs/lstm1_512_cnn5_drop3_5e4_avg_logit_1527855324/model-3800
[Train/3800] Accuracy: 0.941, Loss: 0.317, time/step = 0.061
[Valid/3800] Accuracy: 0.702, Loss: 1.475, time/step = 4.093
[Train/3850] Accuracy: 0.955, Loss: 0.274, time/step = 0.089
[Train/3900] Accuracy: 0.947, Loss: 0.325, time/step = 0.092
[Train/3950] Accuracy: 0.933, Loss: 0.349, time/step = 0.085
[Train/4000] Accuracy: 0.952, Loss: 0.297, time/step = 0.089
[Train/4050] Accuracy: 0.963, Loss: 0.250, time/step = 0.089
[Train/4100] Accuracy: 0.965, Loss: 0.244, time/step = 0.092
[Train/4150] Accuracy: 0.961, Loss: 0.267, time/step = 0.085
[Valid/4180] Accuracy: 0.698, Loss: 1.497, time/step = 4.100
[Train/4200] Accuracy: 0.966, Loss: 0.243, time/step = 0.020
[Train/4250] Accuracy: 0.979, Loss: 0.213, time/step = 0.090
[Train/4300] Accuracy: 0.963, Loss: 0.250, time/step = 0.091
[Train/4350] Accuracy: 0.965, Loss: 0.242, time/step = 0.112
[Train/4400] Accuracy: 0.972, Loss: 0.228, time/step = 0.088
[Train/4450] Accuracy: 0.972, Loss: 0.220, time/step = 0.095
[Train/4500] Accuracy: 0.970, Loss: 0.231, time/step = 0.090
[Train/4550] Accuracy: 0.969, Loss: 0.235, time/step = 0.082
[Valid/4560] Accuracy: 0.702, Loss: 1.552, time/step = 4.049
[Train/4600] Accuracy: 0.947, Loss: 0.303, time/step = 0.094
[Train/4650] Accuracy: 0.971, Loss: 0.229, time/step = 0.077
[Train/4700] Accuracy: 0.971, Loss: 0.225, time/step = 0.094
Model saved in file: /cluster/home/guzhou/MP2018_2D_compare/runs/lstm1_512_cnn5_drop3_5e4_avg_logit_1527855324/model-4750
[Train/4750] Accuracy: 0.967, Loss: 0.236, time/step = 0.063
Model is trained for 25 epochs, 4769 steps.
Done.
Model saved in file: /cluster/home/guzhou/MP2018_2D_compare/runs/lstm1_512_cnn5_drop3_5e4_avg_logit_1527855324/model-4769
Evaluating /cluster/home/guzhou/MP2018_2D_compare/runs/lstm1_512_cnn5_drop3_5e4_avg_logit_1527855324/model-4769
Done.
